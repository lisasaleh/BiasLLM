{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1aeb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Column1', 'text', 'label'],\n",
      "        num_rows: 1811\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Column1', 'text', 'label'],\n",
      "        num_rows: 713\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Column1', 'text', 'label'],\n",
      "        num_rows: 782\n",
      "    })\n",
      "})\n",
      "{'Column1': 8839, 'text': 'mens (crm) oordeelde dat een onderwijsstichting geen verboden onderscheid maakte door een meisje met down syndroom niet door te laten gaan op de reguliere school (oordeelnummer 2011–144).', 'label': 1.0}\n",
      "mens (crm) oordeelde dat een onderwijsstichting geen verboden onderscheid maakte door een meisje met down syndroom niet door te laten gaan op de reguliere school (oordeelnummer 2011–144).\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"milenamileentje/Dutch-Government-Data-for-Bias-detection\")\n",
    "print(data)\n",
    "print(data[\"train\"][0]) \n",
    "print(data[\"train\"][0][\"text\"])\n",
    "print(data[\"train\"][0][\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics1(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    pred_labels = []\n",
    "    for text in decoded_preds:\n",
    "        match = re.search(r\"\\b(0|1)\\b\", text)\n",
    "        pred_labels.append(int(match.group(1)) if match else -1)\n",
    "    \n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    gold_labels = []\n",
    "    for text in decoded_labels:\n",
    "        match = re.search(r\"\\b(0|1)\\b\", text)\n",
    "        gold_labels.append(int(match.group(1)) if match else -1)\n",
    "    for i in range(len(gold_labels)):\n",
    "        print(f\"Predicted: {pred_labels[i]}, Gold: {gold_labels[i]}\")   \n",
    "    \n",
    "    return accuracy.compute(predictions=pred_labels, references=gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc922d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\inakl\\anaconda3\\envs\\atics_group\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\inakl\\anaconda3\\envs\\atics_group\\lib\\site-packages (from scikit-learn) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\inakl\\anaconda3\\envs\\atics_group\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\inakl\\anaconda3\\envs\\atics_group\\lib\\site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\inakl\\anaconda3\\envs\\atics_group\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from transformers import Seq2SeqTrainer\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4557b3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f112a9d922ee41dab4fb6f2d0bd073d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inakl\\AppData\\Local\\Temp\\ipykernel_31952\\1100552167.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "c:\\Users\\inakl\\anaconda3\\envs\\atics_group\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>50.371300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=50.371280670166016, metrics={'train_runtime': 15.111, 'train_samples_per_second': 0.066, 'train_steps_per_second': 0.066, 'total_flos': 1199046131712.0, 'train_loss': 50.371280670166016, 'epoch': 1.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"google/mt5-base\"\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"milenamileentje/Dutch-Government-Data-for-Bias-detection\")\n",
    "\n",
    "def preprocess(example, makeitwords=True):\n",
    "    model_inputs = tokenizer(example['text'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        if makeitwords:\n",
    "            if example[\"label\"] == 0:\n",
    "                label = \"niet-biased\"\n",
    "                labels = tokenizer(label, truncation=True, padding=\"max_length\", max_length=512)\n",
    "            elif example[\"label\"] == 1: \n",
    "                label = \"biased\"\n",
    "                labels = tokenizer(label, truncation=True, padding=\"max_length\", max_length=512)\n",
    "        else:\n",
    "            labels = tokenizer(str(example[\"label\"]), truncation=True, padding=\"max_length\", max_length=512) \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n",
    "    return model_inputs\n",
    "\n",
    "small_train_dataset = dataset[\"train\"].select(range(1))\n",
    "small_val_dataset = dataset[\"validation\"].select(range(1))\n",
    "tokenized_train = small_train_dataset.map(preprocess)\n",
    "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_val = small_val_dataset.map(preprocess)\n",
    "tokenized_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=5e-5,                     \n",
    "    per_device_train_batch_size=1,         \n",
    "    per_device_eval_batch_size=1,         \n",
    "    num_train_epochs=1,                   \n",
    "    weight_decay=0.0,                      \n",
    "    save_strategy=\"no\",                    \n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,                       \n",
    "    report_to=\"none\"\n",
    ")\n",
    "training_args.generation_config = None \n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5bf8e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Column1', 'text', 'label', 'input_ids', 'attention_mask', 'labels', 'decoder_attention_mask'],\n",
      "    num_rows: 1\n",
      "})\n",
      "['<extra_id_0>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train)\n",
    "outputs = model.generate(tokenized_train[\"input_ids\"], attention_mask=tokenized_train[\"attention_mask\"])    \n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atics_group",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
